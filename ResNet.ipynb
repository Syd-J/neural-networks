{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6cf06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jains\\anaconda3\\envs\\DL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3903d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b273fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468b8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b093eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/CIFAR10',\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/CIFAR10',\n",
    "                                           train=False,\n",
    "                                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ceeef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61dd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008ab021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3883b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971f9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d452f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff4a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe21890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/80], Step: [100/500], Loss: 1.5446\n",
      "Epoch: [1/80], Step: [200/500], Loss: 1.5082\n",
      "Epoch: [1/80], Step: [300/500], Loss: 1.1811\n",
      "Epoch: [1/80], Step: [400/500], Loss: 1.1407\n",
      "Epoch: [1/80], Step: [500/500], Loss: 1.0501\n",
      "Epoch: [2/80], Step: [100/500], Loss: 1.0680\n",
      "Epoch: [2/80], Step: [200/500], Loss: 1.1633\n",
      "Epoch: [2/80], Step: [300/500], Loss: 1.2534\n",
      "Epoch: [2/80], Step: [400/500], Loss: 0.9440\n",
      "Epoch: [2/80], Step: [500/500], Loss: 1.0741\n",
      "Epoch: [3/80], Step: [100/500], Loss: 0.8238\n",
      "Epoch: [3/80], Step: [200/500], Loss: 0.9601\n",
      "Epoch: [3/80], Step: [300/500], Loss: 0.7115\n",
      "Epoch: [3/80], Step: [400/500], Loss: 0.7494\n",
      "Epoch: [3/80], Step: [500/500], Loss: 0.8218\n",
      "Epoch: [4/80], Step: [100/500], Loss: 1.0098\n",
      "Epoch: [4/80], Step: [200/500], Loss: 0.9113\n",
      "Epoch: [4/80], Step: [300/500], Loss: 0.7307\n",
      "Epoch: [4/80], Step: [400/500], Loss: 0.8018\n",
      "Epoch: [4/80], Step: [500/500], Loss: 0.7001\n",
      "Epoch: [5/80], Step: [100/500], Loss: 0.7055\n",
      "Epoch: [5/80], Step: [200/500], Loss: 0.5314\n",
      "Epoch: [5/80], Step: [300/500], Loss: 0.6259\n",
      "Epoch: [5/80], Step: [400/500], Loss: 0.5113\n",
      "Epoch: [5/80], Step: [500/500], Loss: 0.5416\n",
      "Epoch: [6/80], Step: [100/500], Loss: 0.4387\n",
      "Epoch: [6/80], Step: [200/500], Loss: 0.5523\n",
      "Epoch: [6/80], Step: [300/500], Loss: 0.6628\n",
      "Epoch: [6/80], Step: [400/500], Loss: 0.6277\n",
      "Epoch: [6/80], Step: [500/500], Loss: 0.7063\n",
      "Epoch: [7/80], Step: [100/500], Loss: 0.6636\n",
      "Epoch: [7/80], Step: [200/500], Loss: 0.6677\n",
      "Epoch: [7/80], Step: [300/500], Loss: 0.6016\n",
      "Epoch: [7/80], Step: [400/500], Loss: 0.4929\n",
      "Epoch: [7/80], Step: [500/500], Loss: 0.5016\n",
      "Epoch: [8/80], Step: [100/500], Loss: 0.4518\n",
      "Epoch: [8/80], Step: [200/500], Loss: 0.5668\n",
      "Epoch: [8/80], Step: [300/500], Loss: 0.5747\n",
      "Epoch: [8/80], Step: [400/500], Loss: 0.4869\n",
      "Epoch: [8/80], Step: [500/500], Loss: 0.4864\n",
      "Epoch: [9/80], Step: [100/500], Loss: 0.4985\n",
      "Epoch: [9/80], Step: [200/500], Loss: 0.5489\n",
      "Epoch: [9/80], Step: [300/500], Loss: 0.6387\n",
      "Epoch: [9/80], Step: [400/500], Loss: 0.3944\n",
      "Epoch: [9/80], Step: [500/500], Loss: 0.5202\n",
      "Epoch: [10/80], Step: [100/500], Loss: 0.5252\n",
      "Epoch: [10/80], Step: [200/500], Loss: 0.4283\n",
      "Epoch: [10/80], Step: [300/500], Loss: 0.4863\n",
      "Epoch: [10/80], Step: [400/500], Loss: 0.3322\n",
      "Epoch: [10/80], Step: [500/500], Loss: 0.6176\n",
      "Epoch: [11/80], Step: [100/500], Loss: 0.4603\n",
      "Epoch: [11/80], Step: [200/500], Loss: 0.6184\n",
      "Epoch: [11/80], Step: [300/500], Loss: 0.4551\n",
      "Epoch: [11/80], Step: [400/500], Loss: 0.5563\n",
      "Epoch: [11/80], Step: [500/500], Loss: 0.5615\n",
      "Epoch: [12/80], Step: [100/500], Loss: 0.4116\n",
      "Epoch: [12/80], Step: [200/500], Loss: 0.3603\n",
      "Epoch: [12/80], Step: [300/500], Loss: 0.5698\n",
      "Epoch: [12/80], Step: [400/500], Loss: 0.4666\n",
      "Epoch: [12/80], Step: [500/500], Loss: 0.3262\n",
      "Epoch: [13/80], Step: [100/500], Loss: 0.4394\n",
      "Epoch: [13/80], Step: [200/500], Loss: 0.4696\n",
      "Epoch: [13/80], Step: [300/500], Loss: 0.4151\n",
      "Epoch: [13/80], Step: [400/500], Loss: 0.5293\n",
      "Epoch: [13/80], Step: [500/500], Loss: 0.4989\n",
      "Epoch: [14/80], Step: [100/500], Loss: 0.4516\n",
      "Epoch: [14/80], Step: [200/500], Loss: 0.3931\n",
      "Epoch: [14/80], Step: [300/500], Loss: 0.3202\n",
      "Epoch: [14/80], Step: [400/500], Loss: 0.4036\n",
      "Epoch: [14/80], Step: [500/500], Loss: 0.4800\n",
      "Epoch: [15/80], Step: [100/500], Loss: 0.4615\n",
      "Epoch: [15/80], Step: [200/500], Loss: 0.4736\n",
      "Epoch: [15/80], Step: [300/500], Loss: 0.2915\n",
      "Epoch: [15/80], Step: [400/500], Loss: 0.4163\n",
      "Epoch: [15/80], Step: [500/500], Loss: 0.4919\n",
      "Epoch: [16/80], Step: [100/500], Loss: 0.4015\n",
      "Epoch: [16/80], Step: [200/500], Loss: 0.2965\n",
      "Epoch: [16/80], Step: [300/500], Loss: 0.4167\n",
      "Epoch: [16/80], Step: [400/500], Loss: 0.3551\n",
      "Epoch: [16/80], Step: [500/500], Loss: 0.3099\n",
      "Epoch: [17/80], Step: [100/500], Loss: 0.3482\n",
      "Epoch: [17/80], Step: [200/500], Loss: 0.4827\n",
      "Epoch: [17/80], Step: [300/500], Loss: 0.4392\n",
      "Epoch: [17/80], Step: [400/500], Loss: 0.3884\n",
      "Epoch: [17/80], Step: [500/500], Loss: 0.2510\n",
      "Epoch: [18/80], Step: [100/500], Loss: 0.3312\n",
      "Epoch: [18/80], Step: [200/500], Loss: 0.4983\n",
      "Epoch: [18/80], Step: [300/500], Loss: 0.3175\n",
      "Epoch: [18/80], Step: [400/500], Loss: 0.4958\n",
      "Epoch: [18/80], Step: [500/500], Loss: 0.3307\n",
      "Epoch: [19/80], Step: [100/500], Loss: 0.3996\n",
      "Epoch: [19/80], Step: [200/500], Loss: 0.4086\n",
      "Epoch: [19/80], Step: [300/500], Loss: 0.3502\n",
      "Epoch: [19/80], Step: [400/500], Loss: 0.2581\n",
      "Epoch: [19/80], Step: [500/500], Loss: 0.3572\n",
      "Epoch: [20/80], Step: [100/500], Loss: 0.4240\n",
      "Epoch: [20/80], Step: [200/500], Loss: 0.3048\n",
      "Epoch: [20/80], Step: [300/500], Loss: 0.3303\n",
      "Epoch: [20/80], Step: [400/500], Loss: 0.3663\n",
      "Epoch: [20/80], Step: [500/500], Loss: 0.4461\n",
      "Epoch: [21/80], Step: [100/500], Loss: 0.4039\n",
      "Epoch: [21/80], Step: [200/500], Loss: 0.2082\n",
      "Epoch: [21/80], Step: [300/500], Loss: 0.2269\n",
      "Epoch: [21/80], Step: [400/500], Loss: 0.3809\n",
      "Epoch: [21/80], Step: [500/500], Loss: 0.2318\n",
      "Epoch: [22/80], Step: [100/500], Loss: 0.3674\n",
      "Epoch: [22/80], Step: [200/500], Loss: 0.3300\n",
      "Epoch: [22/80], Step: [300/500], Loss: 0.4369\n",
      "Epoch: [22/80], Step: [400/500], Loss: 0.2459\n",
      "Epoch: [22/80], Step: [500/500], Loss: 0.2617\n",
      "Epoch: [23/80], Step: [100/500], Loss: 0.2428\n",
      "Epoch: [23/80], Step: [200/500], Loss: 0.2593\n",
      "Epoch: [23/80], Step: [300/500], Loss: 0.3386\n",
      "Epoch: [23/80], Step: [400/500], Loss: 0.2194\n",
      "Epoch: [23/80], Step: [500/500], Loss: 0.2725\n",
      "Epoch: [24/80], Step: [100/500], Loss: 0.2906\n",
      "Epoch: [24/80], Step: [200/500], Loss: 0.2495\n",
      "Epoch: [24/80], Step: [300/500], Loss: 0.2152\n",
      "Epoch: [24/80], Step: [400/500], Loss: 0.3216\n",
      "Epoch: [24/80], Step: [500/500], Loss: 0.3784\n",
      "Epoch: [25/80], Step: [100/500], Loss: 0.1628\n",
      "Epoch: [25/80], Step: [200/500], Loss: 0.2533\n",
      "Epoch: [25/80], Step: [300/500], Loss: 0.2861\n",
      "Epoch: [25/80], Step: [400/500], Loss: 0.1348\n",
      "Epoch: [25/80], Step: [500/500], Loss: 0.2518\n",
      "Epoch: [26/80], Step: [100/500], Loss: 0.1507\n",
      "Epoch: [26/80], Step: [200/500], Loss: 0.1953\n",
      "Epoch: [26/80], Step: [300/500], Loss: 0.2728\n",
      "Epoch: [26/80], Step: [400/500], Loss: 0.4439\n",
      "Epoch: [26/80], Step: [500/500], Loss: 0.1285\n",
      "Epoch: [27/80], Step: [100/500], Loss: 0.2537\n",
      "Epoch: [27/80], Step: [200/500], Loss: 0.2089\n",
      "Epoch: [27/80], Step: [300/500], Loss: 0.3998\n",
      "Epoch: [27/80], Step: [400/500], Loss: 0.2723\n",
      "Epoch: [27/80], Step: [500/500], Loss: 0.2634\n",
      "Epoch: [28/80], Step: [100/500], Loss: 0.1939\n",
      "Epoch: [28/80], Step: [200/500], Loss: 0.2934\n",
      "Epoch: [28/80], Step: [300/500], Loss: 0.3486\n",
      "Epoch: [28/80], Step: [400/500], Loss: 0.2522\n",
      "Epoch: [28/80], Step: [500/500], Loss: 0.1809\n",
      "Epoch: [29/80], Step: [100/500], Loss: 0.2611\n",
      "Epoch: [29/80], Step: [200/500], Loss: 0.1879\n",
      "Epoch: [29/80], Step: [300/500], Loss: 0.1738\n",
      "Epoch: [29/80], Step: [400/500], Loss: 0.2342\n",
      "Epoch: [29/80], Step: [500/500], Loss: 0.2159\n",
      "Epoch: [30/80], Step: [100/500], Loss: 0.2547\n",
      "Epoch: [30/80], Step: [200/500], Loss: 0.1337\n",
      "Epoch: [30/80], Step: [300/500], Loss: 0.1098\n",
      "Epoch: [30/80], Step: [400/500], Loss: 0.3141\n",
      "Epoch: [30/80], Step: [500/500], Loss: 0.2461\n",
      "Epoch: [31/80], Step: [100/500], Loss: 0.1762\n",
      "Epoch: [31/80], Step: [200/500], Loss: 0.1369\n",
      "Epoch: [31/80], Step: [300/500], Loss: 0.2452\n",
      "Epoch: [31/80], Step: [400/500], Loss: 0.2071\n",
      "Epoch: [31/80], Step: [500/500], Loss: 0.2352\n",
      "Epoch: [32/80], Step: [100/500], Loss: 0.2230\n",
      "Epoch: [32/80], Step: [200/500], Loss: 0.2505\n",
      "Epoch: [32/80], Step: [300/500], Loss: 0.2543\n",
      "Epoch: [32/80], Step: [400/500], Loss: 0.3438\n",
      "Epoch: [32/80], Step: [500/500], Loss: 0.1629\n",
      "Epoch: [33/80], Step: [100/500], Loss: 0.2674\n",
      "Epoch: [33/80], Step: [200/500], Loss: 0.2064\n",
      "Epoch: [33/80], Step: [300/500], Loss: 0.2235\n",
      "Epoch: [33/80], Step: [400/500], Loss: 0.1354\n",
      "Epoch: [33/80], Step: [500/500], Loss: 0.1949\n",
      "Epoch: [34/80], Step: [100/500], Loss: 0.2048\n",
      "Epoch: [34/80], Step: [200/500], Loss: 0.1952\n",
      "Epoch: [34/80], Step: [300/500], Loss: 0.3238\n",
      "Epoch: [34/80], Step: [400/500], Loss: 0.1094\n",
      "Epoch: [34/80], Step: [500/500], Loss: 0.1552\n",
      "Epoch: [35/80], Step: [100/500], Loss: 0.2443\n",
      "Epoch: [35/80], Step: [200/500], Loss: 0.1560\n",
      "Epoch: [35/80], Step: [300/500], Loss: 0.2640\n",
      "Epoch: [35/80], Step: [400/500], Loss: 0.2003\n",
      "Epoch: [35/80], Step: [500/500], Loss: 0.3140\n",
      "Epoch: [36/80], Step: [100/500], Loss: 0.2149\n",
      "Epoch: [36/80], Step: [200/500], Loss: 0.1756\n",
      "Epoch: [36/80], Step: [300/500], Loss: 0.2745\n",
      "Epoch: [36/80], Step: [400/500], Loss: 0.1999\n",
      "Epoch: [36/80], Step: [500/500], Loss: 0.1871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37/80], Step: [100/500], Loss: 0.2379\n",
      "Epoch: [37/80], Step: [200/500], Loss: 0.3062\n",
      "Epoch: [37/80], Step: [300/500], Loss: 0.2138\n",
      "Epoch: [37/80], Step: [400/500], Loss: 0.1040\n",
      "Epoch: [37/80], Step: [500/500], Loss: 0.2208\n",
      "Epoch: [38/80], Step: [100/500], Loss: 0.2139\n",
      "Epoch: [38/80], Step: [200/500], Loss: 0.2344\n",
      "Epoch: [38/80], Step: [300/500], Loss: 0.2208\n",
      "Epoch: [38/80], Step: [400/500], Loss: 0.1653\n",
      "Epoch: [38/80], Step: [500/500], Loss: 0.2203\n",
      "Epoch: [39/80], Step: [100/500], Loss: 0.2694\n",
      "Epoch: [39/80], Step: [200/500], Loss: 0.2089\n",
      "Epoch: [39/80], Step: [300/500], Loss: 0.1760\n",
      "Epoch: [39/80], Step: [400/500], Loss: 0.1721\n",
      "Epoch: [39/80], Step: [500/500], Loss: 0.3229\n",
      "Epoch: [40/80], Step: [100/500], Loss: 0.2006\n",
      "Epoch: [40/80], Step: [200/500], Loss: 0.2823\n",
      "Epoch: [40/80], Step: [300/500], Loss: 0.1574\n",
      "Epoch: [40/80], Step: [400/500], Loss: 0.2523\n",
      "Epoch: [40/80], Step: [500/500], Loss: 0.3011\n",
      "Epoch: [41/80], Step: [100/500], Loss: 0.2721\n",
      "Epoch: [41/80], Step: [200/500], Loss: 0.1584\n",
      "Epoch: [41/80], Step: [300/500], Loss: 0.1822\n",
      "Epoch: [41/80], Step: [400/500], Loss: 0.1958\n",
      "Epoch: [41/80], Step: [500/500], Loss: 0.2434\n",
      "Epoch: [42/80], Step: [100/500], Loss: 0.1868\n",
      "Epoch: [42/80], Step: [200/500], Loss: 0.2318\n",
      "Epoch: [42/80], Step: [300/500], Loss: 0.1328\n",
      "Epoch: [42/80], Step: [400/500], Loss: 0.2278\n",
      "Epoch: [42/80], Step: [500/500], Loss: 0.2600\n",
      "Epoch: [43/80], Step: [100/500], Loss: 0.1266\n",
      "Epoch: [43/80], Step: [200/500], Loss: 0.1423\n",
      "Epoch: [43/80], Step: [300/500], Loss: 0.1793\n",
      "Epoch: [43/80], Step: [400/500], Loss: 0.1054\n",
      "Epoch: [43/80], Step: [500/500], Loss: 0.2486\n",
      "Epoch: [44/80], Step: [100/500], Loss: 0.1699\n",
      "Epoch: [44/80], Step: [200/500], Loss: 0.2494\n",
      "Epoch: [44/80], Step: [300/500], Loss: 0.2409\n",
      "Epoch: [44/80], Step: [400/500], Loss: 0.1962\n",
      "Epoch: [44/80], Step: [500/500], Loss: 0.2033\n",
      "Epoch: [45/80], Step: [100/500], Loss: 0.1380\n",
      "Epoch: [45/80], Step: [200/500], Loss: 0.1312\n",
      "Epoch: [45/80], Step: [300/500], Loss: 0.1830\n",
      "Epoch: [45/80], Step: [400/500], Loss: 0.1165\n",
      "Epoch: [45/80], Step: [500/500], Loss: 0.1939\n",
      "Epoch: [46/80], Step: [100/500], Loss: 0.2727\n",
      "Epoch: [46/80], Step: [200/500], Loss: 0.1239\n",
      "Epoch: [46/80], Step: [300/500], Loss: 0.3608\n",
      "Epoch: [46/80], Step: [400/500], Loss: 0.1143\n",
      "Epoch: [46/80], Step: [500/500], Loss: 0.3008\n",
      "Epoch: [47/80], Step: [100/500], Loss: 0.1808\n",
      "Epoch: [47/80], Step: [200/500], Loss: 0.1418\n",
      "Epoch: [47/80], Step: [300/500], Loss: 0.1314\n",
      "Epoch: [47/80], Step: [400/500], Loss: 0.2431\n",
      "Epoch: [47/80], Step: [500/500], Loss: 0.1960\n",
      "Epoch: [48/80], Step: [100/500], Loss: 0.1740\n",
      "Epoch: [48/80], Step: [200/500], Loss: 0.1719\n",
      "Epoch: [48/80], Step: [300/500], Loss: 0.1630\n",
      "Epoch: [48/80], Step: [400/500], Loss: 0.2623\n",
      "Epoch: [48/80], Step: [500/500], Loss: 0.0734\n",
      "Epoch: [49/80], Step: [100/500], Loss: 0.1104\n",
      "Epoch: [49/80], Step: [200/500], Loss: 0.1120\n",
      "Epoch: [49/80], Step: [300/500], Loss: 0.1627\n",
      "Epoch: [49/80], Step: [400/500], Loss: 0.2473\n",
      "Epoch: [49/80], Step: [500/500], Loss: 0.1704\n",
      "Epoch: [50/80], Step: [100/500], Loss: 0.1398\n",
      "Epoch: [50/80], Step: [200/500], Loss: 0.1687\n",
      "Epoch: [50/80], Step: [300/500], Loss: 0.2460\n",
      "Epoch: [50/80], Step: [400/500], Loss: 0.1696\n",
      "Epoch: [50/80], Step: [500/500], Loss: 0.1993\n",
      "Epoch: [51/80], Step: [100/500], Loss: 0.2024\n",
      "Epoch: [51/80], Step: [200/500], Loss: 0.1678\n",
      "Epoch: [51/80], Step: [300/500], Loss: 0.1999\n",
      "Epoch: [51/80], Step: [400/500], Loss: 0.2224\n",
      "Epoch: [51/80], Step: [500/500], Loss: 0.2195\n",
      "Epoch: [52/80], Step: [100/500], Loss: 0.1601\n",
      "Epoch: [52/80], Step: [200/500], Loss: 0.1537\n",
      "Epoch: [52/80], Step: [300/500], Loss: 0.2783\n",
      "Epoch: [52/80], Step: [400/500], Loss: 0.1050\n",
      "Epoch: [52/80], Step: [500/500], Loss: 0.1568\n",
      "Epoch: [53/80], Step: [100/500], Loss: 0.1839\n",
      "Epoch: [53/80], Step: [200/500], Loss: 0.1537\n",
      "Epoch: [53/80], Step: [300/500], Loss: 0.1289\n",
      "Epoch: [53/80], Step: [400/500], Loss: 0.2098\n",
      "Epoch: [53/80], Step: [500/500], Loss: 0.1667\n",
      "Epoch: [54/80], Step: [100/500], Loss: 0.1876\n",
      "Epoch: [54/80], Step: [200/500], Loss: 0.1828\n",
      "Epoch: [54/80], Step: [300/500], Loss: 0.1552\n",
      "Epoch: [54/80], Step: [400/500], Loss: 0.1176\n",
      "Epoch: [54/80], Step: [500/500], Loss: 0.2826\n",
      "Epoch: [55/80], Step: [100/500], Loss: 0.1815\n",
      "Epoch: [55/80], Step: [200/500], Loss: 0.2115\n",
      "Epoch: [55/80], Step: [300/500], Loss: 0.1751\n",
      "Epoch: [55/80], Step: [400/500], Loss: 0.2238\n",
      "Epoch: [55/80], Step: [500/500], Loss: 0.1802\n",
      "Epoch: [56/80], Step: [100/500], Loss: 0.1202\n",
      "Epoch: [56/80], Step: [200/500], Loss: 0.1159\n",
      "Epoch: [56/80], Step: [300/500], Loss: 0.1006\n",
      "Epoch: [56/80], Step: [400/500], Loss: 0.1802\n",
      "Epoch: [56/80], Step: [500/500], Loss: 0.2974\n",
      "Epoch: [57/80], Step: [100/500], Loss: 0.1352\n",
      "Epoch: [57/80], Step: [200/500], Loss: 0.2066\n",
      "Epoch: [57/80], Step: [300/500], Loss: 0.1536\n",
      "Epoch: [57/80], Step: [400/500], Loss: 0.1048\n",
      "Epoch: [57/80], Step: [500/500], Loss: 0.2535\n",
      "Epoch: [58/80], Step: [100/500], Loss: 0.0876\n",
      "Epoch: [58/80], Step: [200/500], Loss: 0.1717\n",
      "Epoch: [58/80], Step: [300/500], Loss: 0.0691\n",
      "Epoch: [58/80], Step: [400/500], Loss: 0.1537\n",
      "Epoch: [58/80], Step: [500/500], Loss: 0.1723\n",
      "Epoch: [59/80], Step: [100/500], Loss: 0.2141\n",
      "Epoch: [59/80], Step: [200/500], Loss: 0.1951\n",
      "Epoch: [59/80], Step: [300/500], Loss: 0.2096\n",
      "Epoch: [59/80], Step: [400/500], Loss: 0.1068\n",
      "Epoch: [59/80], Step: [500/500], Loss: 0.2247\n",
      "Epoch: [60/80], Step: [100/500], Loss: 0.0955\n",
      "Epoch: [60/80], Step: [200/500], Loss: 0.2710\n",
      "Epoch: [60/80], Step: [300/500], Loss: 0.1959\n",
      "Epoch: [60/80], Step: [400/500], Loss: 0.1178\n",
      "Epoch: [60/80], Step: [500/500], Loss: 0.1510\n",
      "Epoch: [61/80], Step: [100/500], Loss: 0.0996\n",
      "Epoch: [61/80], Step: [200/500], Loss: 0.2011\n",
      "Epoch: [61/80], Step: [300/500], Loss: 0.1452\n",
      "Epoch: [61/80], Step: [400/500], Loss: 0.2566\n",
      "Epoch: [61/80], Step: [500/500], Loss: 0.2046\n",
      "Epoch: [62/80], Step: [100/500], Loss: 0.1370\n",
      "Epoch: [62/80], Step: [200/500], Loss: 0.0987\n",
      "Epoch: [62/80], Step: [300/500], Loss: 0.1587\n",
      "Epoch: [62/80], Step: [400/500], Loss: 0.1201\n",
      "Epoch: [62/80], Step: [500/500], Loss: 0.1683\n",
      "Epoch: [63/80], Step: [100/500], Loss: 0.1745\n",
      "Epoch: [63/80], Step: [200/500], Loss: 0.2184\n",
      "Epoch: [63/80], Step: [300/500], Loss: 0.1088\n",
      "Epoch: [63/80], Step: [400/500], Loss: 0.2256\n",
      "Epoch: [63/80], Step: [500/500], Loss: 0.2069\n",
      "Epoch: [64/80], Step: [100/500], Loss: 0.1721\n",
      "Epoch: [64/80], Step: [200/500], Loss: 0.1220\n",
      "Epoch: [64/80], Step: [300/500], Loss: 0.0883\n",
      "Epoch: [64/80], Step: [400/500], Loss: 0.1264\n",
      "Epoch: [64/80], Step: [500/500], Loss: 0.1405\n",
      "Epoch: [65/80], Step: [100/500], Loss: 0.1518\n",
      "Epoch: [65/80], Step: [200/500], Loss: 0.1638\n",
      "Epoch: [65/80], Step: [300/500], Loss: 0.1405\n",
      "Epoch: [65/80], Step: [400/500], Loss: 0.1704\n",
      "Epoch: [65/80], Step: [500/500], Loss: 0.1404\n",
      "Epoch: [66/80], Step: [100/500], Loss: 0.1529\n",
      "Epoch: [66/80], Step: [200/500], Loss: 0.1141\n",
      "Epoch: [66/80], Step: [300/500], Loss: 0.1764\n",
      "Epoch: [66/80], Step: [400/500], Loss: 0.1865\n",
      "Epoch: [66/80], Step: [500/500], Loss: 0.0672\n",
      "Epoch: [67/80], Step: [100/500], Loss: 0.1864\n",
      "Epoch: [67/80], Step: [200/500], Loss: 0.1630\n",
      "Epoch: [67/80], Step: [300/500], Loss: 0.1856\n",
      "Epoch: [67/80], Step: [400/500], Loss: 0.1855\n",
      "Epoch: [67/80], Step: [500/500], Loss: 0.0958\n",
      "Epoch: [68/80], Step: [100/500], Loss: 0.1538\n",
      "Epoch: [68/80], Step: [200/500], Loss: 0.1683\n",
      "Epoch: [68/80], Step: [300/500], Loss: 0.1051\n",
      "Epoch: [68/80], Step: [400/500], Loss: 0.1156\n",
      "Epoch: [68/80], Step: [500/500], Loss: 0.0716\n",
      "Epoch: [69/80], Step: [100/500], Loss: 0.1344\n",
      "Epoch: [69/80], Step: [200/500], Loss: 0.0908\n",
      "Epoch: [69/80], Step: [300/500], Loss: 0.2025\n",
      "Epoch: [69/80], Step: [400/500], Loss: 0.0842\n",
      "Epoch: [69/80], Step: [500/500], Loss: 0.1627\n",
      "Epoch: [70/80], Step: [100/500], Loss: 0.1673\n",
      "Epoch: [70/80], Step: [200/500], Loss: 0.1560\n",
      "Epoch: [70/80], Step: [300/500], Loss: 0.1920\n",
      "Epoch: [70/80], Step: [400/500], Loss: 0.1575\n",
      "Epoch: [70/80], Step: [500/500], Loss: 0.1258\n",
      "Epoch: [71/80], Step: [100/500], Loss: 0.0797\n",
      "Epoch: [71/80], Step: [200/500], Loss: 0.1303\n",
      "Epoch: [71/80], Step: [300/500], Loss: 0.1053\n",
      "Epoch: [71/80], Step: [400/500], Loss: 0.1099\n",
      "Epoch: [71/80], Step: [500/500], Loss: 0.1059\n",
      "Epoch: [72/80], Step: [100/500], Loss: 0.1047\n",
      "Epoch: [72/80], Step: [200/500], Loss: 0.1440\n",
      "Epoch: [72/80], Step: [300/500], Loss: 0.0982\n",
      "Epoch: [72/80], Step: [400/500], Loss: 0.0873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72/80], Step: [500/500], Loss: 0.1273\n",
      "Epoch: [73/80], Step: [100/500], Loss: 0.0910\n",
      "Epoch: [73/80], Step: [200/500], Loss: 0.1597\n",
      "Epoch: [73/80], Step: [300/500], Loss: 0.1364\n",
      "Epoch: [73/80], Step: [400/500], Loss: 0.0860\n",
      "Epoch: [73/80], Step: [500/500], Loss: 0.1874\n",
      "Epoch: [74/80], Step: [100/500], Loss: 0.1447\n",
      "Epoch: [74/80], Step: [200/500], Loss: 0.1439\n",
      "Epoch: [74/80], Step: [300/500], Loss: 0.0921\n",
      "Epoch: [74/80], Step: [400/500], Loss: 0.0689\n",
      "Epoch: [74/80], Step: [500/500], Loss: 0.1902\n",
      "Epoch: [75/80], Step: [100/500], Loss: 0.1325\n",
      "Epoch: [75/80], Step: [200/500], Loss: 0.1033\n",
      "Epoch: [75/80], Step: [300/500], Loss: 0.0896\n",
      "Epoch: [75/80], Step: [400/500], Loss: 0.1741\n",
      "Epoch: [75/80], Step: [500/500], Loss: 0.1497\n",
      "Epoch: [76/80], Step: [100/500], Loss: 0.0923\n",
      "Epoch: [76/80], Step: [200/500], Loss: 0.1338\n",
      "Epoch: [76/80], Step: [300/500], Loss: 0.0738\n",
      "Epoch: [76/80], Step: [400/500], Loss: 0.1662\n",
      "Epoch: [76/80], Step: [500/500], Loss: 0.0775\n",
      "Epoch: [77/80], Step: [100/500], Loss: 0.1431\n",
      "Epoch: [77/80], Step: [200/500], Loss: 0.1702\n",
      "Epoch: [77/80], Step: [300/500], Loss: 0.2032\n",
      "Epoch: [77/80], Step: [400/500], Loss: 0.2295\n",
      "Epoch: [77/80], Step: [500/500], Loss: 0.1132\n",
      "Epoch: [78/80], Step: [100/500], Loss: 0.1368\n",
      "Epoch: [78/80], Step: [200/500], Loss: 0.1259\n",
      "Epoch: [78/80], Step: [300/500], Loss: 0.1315\n",
      "Epoch: [78/80], Step: [400/500], Loss: 0.1249\n",
      "Epoch: [78/80], Step: [500/500], Loss: 0.1931\n",
      "Epoch: [79/80], Step: [100/500], Loss: 0.1236\n",
      "Epoch: [79/80], Step: [200/500], Loss: 0.0651\n",
      "Epoch: [79/80], Step: [300/500], Loss: 0.0852\n",
      "Epoch: [79/80], Step: [400/500], Loss: 0.2155\n",
      "Epoch: [79/80], Step: [500/500], Loss: 0.0912\n",
      "Epoch: [80/80], Step: [100/500], Loss: 0.1163\n",
      "Epoch: [80/80], Step: [200/500], Loss: 0.1108\n",
      "Epoch: [80/80], Step: [300/500], Loss: 0.1728\n",
      "Epoch: [80/80], Step: [400/500], Loss: 0.1890\n",
      "Epoch: [80/80], Step: [500/500], Loss: 0.1485\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dc76b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 88.45 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
